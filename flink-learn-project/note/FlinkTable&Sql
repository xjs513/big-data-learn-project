创建 TableEnviroment
    val tableEnv = StreamTableEnvironment.create(env)
TableEnviroment 是 Flink 中 Table API 和 SQL 的核心概念, 所有表操作都基于此。
-- 注册 Catalog
-- 在 Catalog 中注册表
-- 执行 SQL 查询
-- 注册UDF

表 (Table)
* TableEnvironment 可以注册 Catalog, 然后基于 Catalog 注册表
* 表[Table]是由一个"标识符"(identifier)来指定的,
  由Catalog名、数据库(database)名和对象名三部分组成
* 表可以是常规的, 也可以是虚拟的(View)
* 常规表一般可以用来描述外部数据,比如文件、数据库表或消息队列的数据,
  也可以直接从DataStream转换而来
* 视图可以从现有表创建,通常是 Table API 或 SQL 查询的一个结果集

创建表
* TableEnvironment可以调用.connect()方法连接外部系统,
  并调用.createTemporaryTable()方法在Catalog中注册表
    tableEnv
        .connect(...)                           // 定义表的数据来源,和外部系统建立连接
        .withFormat(...)                        // 定义数据格式化方法
        .withSchema(...)                        // 定义表结构
        .createTemporaryTable("MyTable")        // 创建临时表

表的查询 - Table API  P72
* Table API 是集成在 Scala 和 Java 语言内的查询 API
* Table API 基于代表表的 Table 类, 并提供一套操作处理的 API;
  这些方法会返回一个新的 Table 对象, 表示对输入表应用转换操作的结果
* 有些关系型转换操作, 可以由多个方法调用组成, 构成链式调用结构
  val sensorTable:Table = tableEnv.from("inputTable")
  val resultTable:Table = sensorTable
                            .select("id, temperature")
                            .filter("id = 'sensor_1'")

表的查询 - SQL
* Flink 的 SQL 集成, 基于实现了 SQL 标准的 Apache Calcite
* Flink 中, 用常规字符串定义 SQL 查询语句
* SQL 查询结果也是一个新的 Table
  val resultSqlTable:Table = tableEnv
        .sqlQuery("select id, temperature fron sensorTable where id ='sensor_1'")

DataStream 转换成表 P73
* DataStream 可以直接转换成 Table, 进而调用 Table API 进行操作
  val dataTable:Table = tableEnv.fromDataStream(dataStream)
* 默认转换后的 Table Schema 和 DataStream 的字段定义一一对应, 也可单独指定
  val dataStream:DataStream[SensorReading] = ...
  val sensorTable = tableEnv.fromDataStream(
    dataStream,
    'id, 'timestamp, 'temperature
  )

数据类型与 Schema 的对应
* DataStream 的数据类型与表的 Schema 之间的对应关系, 可以有两种:
  基于字段名、或者基于字段位置

* 基于名称 - name-based
  val sensorTable = tableEnv.fromDataStream(
    dataStream,
    'timestamp as 'ts, 'id as 'myId, 'temperature
  )

* 基于位置 - position-based
  * 基于名称 - name-based
    val sensorTable = tableEnv.fromDataStream(
      dataStream, 'myId, 'ts
    )

创建临时视图 Temporary View
* 基于 DataStream 创建临时视图
  tableEnv.createTemporaryView("sensorView", dataStream)

  tableEnv.createTemporaryView("sensorView", dataStream,
    'id, 'temperature, 'timestamp as 'ts
  )
* 基于 Table 创建临时视图
  tableEnv.createTemporaryView("sensorView", sensorTable)


输出表 P74
* 表的输出, 通过将数据写入 TableSink 来实现
* TableSink 是一个通用接口, 可以支持不同的文件格式、数据库和消息队列
* 输出表最直接的方法, 就是通过 Table.insertInto() 方法将一个 Table
  写入注册过的 TableSink 中
  tableEnv.connect(...)
    .createTemporaryTable("outputTable")
  val resultSqlTable:Table = ....
  resultSqlTable.insertInto("outputTable")

输出到文件, 见测试源码  Append 和  Retract 的区别

更新模式 P75  Update Mode
* 对于流式查询, 需要声明在表和外部连接器之间如何执行转换
* 与外部系统交换的消息类型, 由更新模式指定
  > 追加模式  Append
    - 表只做插入操作, 和外部连接器只交换插入（insert）消息
  > 撤回模式  Retract
    - 表和外部连接器交换添加(add)和撤回(retract)消息
    - 插入操作(insert)编码为 Add 消息; 删除(delete)编码为 Retract 消息;
      更新(Update)编码为上一条的 Retract 和下一条的 Add 消息
  > 更新插入模式  Upset
    - 更新和插入都被编码为 Upset 消息; 删除编码为 Delete 消息
      根据 key 有没有决定是更新还是插入
      好处是比撤回模式下更新操作发送消息减少
      缺点是外部系统要设置 key

输出到 Kafka、MySQL、ES都见测试代码

将 Table 转换成 DataStream P79
* 表可以转换为 DataStream 或 DataSet, 这样自定义流处理或批处理程序
  就可以继续在 Table API 或 SQL 查询的结果集上运行了
* 将表转换为 DataStream 或 DataSet时, 需要指定泛型,
  即将表的每一行转换成的数据类型 可以是元组或 Row
* 表作为流式查询的结果, 是动态更新的
* 有两种转化安模式: 追加(Append)模式和撤回(Retract)模式

查看执行计划  P80
* Table API 提供了一种机制来解释计划表的逻辑和优化查询计划
* 查看执行计划, 可以通过 TableEnvironment.explain(table)方法或
  TableEnvironment.explain()方法完成, 返回一个字符串, 描述三个计划
  > 抽象语法树
  > 优化后的逻辑查询计划
  > 实际执行计划
  val explaination:String = tableEnv.explain(resultTable)
  println(explaination)

流处理和关系代数的区别

                关系代数(表)/SQL             流处理

处理的数据对象     字段元组的有界集合         字段元组的无限序列

查询对数据的访问   可以访问到完整的数据输入    无法访问所欲数据,必须持续等待流式输入

查询终止条件      生成固定大小的结果集后终止   永不停止, 持续接收数据不断更新查询结果


时间特性(Time Attributes)
* 基于时间的操作(比如 Table API 和 SQL 中窗口操作),
  需要定义相关的时间语义和时间数据来源的信息
* Table 可以提供一个逻辑上的时间字段, 用于在表处理程序中,
  指示时间和访问相应的时间戳
* 时间属性, 可以是每个表 Schema 的一部分。一旦定义了时间属性,
  就可以作为一个字段,在基于时间的操作中使用。
* 时间属性的行为类似于常规时间戳,可以访问并进行计算

定义处理时间 (Processing Time)
* 处理时间语义下, 允许表处理程序根据机器本地时间生成结果。
  是时间的最简单概念,不需要提取时间戳和生成水印。
* 几种不同的定义处理时间的方法:
  > 1. DataStream 转换成 Table 时指定
    val sensorTable = tableEnv.fromDataStream(dataStream
        'id, 'temperature, 'timestamp, 'pt.proctime)
    在定义 Schema 期间, 可以使用 .proctime, 指定字段名定义处理时间
    这个 proctime 属性只能通过附加逻辑字段,来扩展物理 Schema。
    因此只能在 Schema 的末尾定义。
    val sensorTable = tableEnv.fromDataStream(dataStream,$"id", $"timestamp", $"temperature", $"pt".proctime)
  > 2. 定义 Table Schema 时指定
    tableEnv
      .connect(new FileSystem().path(filePath))
      .withFormat(new Csv())
      .withSchema(
        new Schema()
          .field("id", DataTypes.STRING())
          .field("timestamp", DataTypes.BIGINT())
          .field("temperature", DataTypes.DOUBLE())
          .field("pt", DataTypes.TIMESTAMP(3)).proctime()
      )
      .createTemporaryTable("inputTable")

      # 但是: 文件指定 proctime 会报错
      CsvTableSource 没有实现相关接口:
      KafkaTableSourceBase 实现了相关接口:
         DefinedProctimeAttribute
         DefinedRowtimeAttributes
  > 3. 在创建表的 DDL 中定义
    val sinkDDL 中定义:
       追加一个字段, 计算列的概念
       pt AS PROCTIME()

定义事件时间  ** 这个是重点  P84
* 事件时间语义, 允许包处理程序根据每个记录中包含的时间生成结果。
  这样即使在有乱序事件或者延迟事件时,也可以得到正确结果。
* 为了处理无序事件,并区分流中的准时和迟到事件;Flink 需要从事件数据中,
  提取时间戳和生成水印,并用来推进事件时间的进展
* 定义事件时间,同样有三种方法:
  > 1. DataStream 转换成 Table 时指定
    ** 需要提前为 DataStream 提取时间戳和分配水印
    // extract timestamp and assign watermarks based on knowledge of the stream
    使用 .rowtime, 可以定义事件时间属性
    # 指定特定字段为事件时间
    val sensorTable = tableEnv.fromDataStream(dataStream,
        $"id", $"timestamp".rowtime, $"temperature")
    # 或者直接追加时间字段
    val sensorTable = tableEnv.fromDataStream(dataStream,
            $"id", $"temperature", $"timestamp", $"rt".rowtime)
  > 2. 定义 Table Schema 时指定
    tableEnv
      .connect(new FileSystem().path(filePath))
      .withFormat(new Csv())
      .withSchema(
        new Schema()
          .field("id", DataTypes.STRING())
          .field("timestamp", DataTypes.BIGINT())
          .rowtime(
            new Rowtime()
                .timestampFromField("timestamp") // 从字段提取时间戳
                .watermarksPeriodicBounded(1000) // 水印延迟 1 秒
          )
          .field("temperature", DataTypes.DOUBLE())
      )
      .createTemporaryTable("inputTable")

      # 但是: 文件指定 proctime 会报错
      CsvTableSource 没有实现相关接口:
      KafkaTableSourceBase 实现了相关接口:
         DefinedProctimeAttribute
         DefinedRowtimeAttributes
  > 3. 在创建表的 DDL 中定义
    val sinkDDL 中定义:
       追加一个字段, 计算列的概念
       ts bigint,
       rt AS TO_TIMESTAMP(FROM_UNIXTIME(ts)),
       watermark for rt as rt - interval '1' second
    官方文档定义方式:
    CREATE TABLE user_actions (
      user_name STRING,
      data STRING,
      user_action_time TIMESTAMP(3),
      -- declare user_action_time as event time attribute and use 5 seconds delayed watermark strategy
      WATERMARK FOR user_action_time AS user_action_time - INTERVAL '5' SECOND
    ) WITH (
      ...
    );

窗口 P85
* 时间语义要配合窗口操作才能发挥作用
* 在 Table API和 SQL 中, 主要有两种窗口
  > Group Windows 分组窗口
    - 根据时间或行计数间隔, 将行聚合到有限的组(Group)中,并对组内数据执行一次聚合函数
  > Over Windows 开窗函数
    - 针对每个输入行,计算相邻行范围内数据的聚合

Group Windows
* Group Windows 使用 .window(w:GroupWindow as w) 子句定义,且必须用 as 指定别名。
* 为了按窗口对表进行分组, 窗口别名必须在 group by 子句内,像常规分组字段一样引用
  val table =
    input
     .window([w:GroupWindow] as 'w')
     .groupBy('w, 'a)    // 按照字段 a 和窗口 w 分组
     .select('a, 'b.sum) // 聚合

----------------------------------------------------------------------------
* Table API 提供了一组具有特定语义的预定义 Window 类,
  这些类会被转换为底层 DataStream 或 DataSet 的窗口操作

滚动窗口 Tumbling windows
* 滚动窗口用 Tumble 类来定义
// Tumbling Event-time Window
.window(Tumble over 10.minutes on 'rowtime as 'w)
// Tumbling Processing-time Window
.window(Tumble over 10.minutes on 'proctime as 'w)
// Tumbling Row-count Window
.window(Tumble over 10.rows on 'proctime as 'w)

滑动窗口 Sliding windows
* 滑动窗口 Slide 类来定义
// Sliding Event-time Window
.window(Slide over 10.minutes every 5.minutes on 'rowtime as 'w)
// Sliding Processing-time Window
.window(Slide over 10.minutes every 5.minutes on 'proctime as 'w)
// Sliding Row-count Window
.window(Slide over 10.rows every 5.rows on 'proctime as 'w)

会话窗口 Session windows
* 会话窗口用 Session 类定义
// Session Event-time Window
.window(Session withGap 10.minutes on 'rowtime as 'w)
// Session Processing-time Window
.window(Session withGap 10.minutes on 'proctime as 'w)
----------------------------------------------------------------------------
SQL 中的 Group Windows
* Group Windows 定义在 SQL 查询的 Group By 子句中
  > TUMBLE(time_attr, interval)
    定义一个滚动窗口, time_attr 是时间字段, interval 是窗口长度
  > HOP(time_attr, interval, interval)
    定义一个滑动窗口, time_attr 是时间字段, 窗口滑动步长, 窗口长度
  > SESSION(time_attr, interval)
    定义一个会话窗口, time_attr 是时间字段, interval 是窗口间隔
val resultSQLTalbe = tableEnv.sqlQuery(
  """
    |select
    | id,
    | count(id),
    | avg(temperature),
    | TUMBLE_START(rt, INTERVAL '10' second) AS wStarata,
    | TUMBLE_END(rt, INTERVAL '10' second)  AS wEnd
    |from
    | sensor
    |group by
    | id,
    | TUMBLE(rt, INTERVAL '10' second)
  """.stripMargin)
----------------------------------------------------------------------------
Over Windows
* Overe window 聚合是标准 SQL 中已有的(over 子句),可以在查询的 SELECT 子句中定义
* Overe window 聚合会针对每个输入行, 计算相邻行范围内的聚合
* Overe windows 使用 window(w:overwindows*) 子句定义,
  并在 select() 方法中通过别名引用
  val table =
    input
      .window([w:Overwindow] as $"w")
      .select($"a", $"b".sum over $"w", $"c".min over $"w")
----------------------------------------------------------------------------
Table API 提供了 Over 类, 来配置 Over 窗口的属性

无界 Over Windows
* 可以在事件时间或处理时间, 以及指定为时间间隔或行计数的范围内,定义 Over window
* 无界的 over window 使用常量指定
// 无界的事件时间 over window
.window(Over partitionBy $"a" orderBy $"rowtime" preceding UNBOUNDED_RANGE as $"w")
// 无界的处理时间 over window
.window(Over partitionBy $"a" orderBy $"proctime" preceding UNBOUNDED_RANGE as $"w")
// 无界的事件时间 Row-count over window
.window(Over partitionBy $"a" orderBy $"rowtime" preceding UNBOUNDED_ROW as $"w")
// 无界的处理时间 Row-count over window
.window(Over partitionBy $"a" orderBy $"proctime" preceding UNBOUNDED_ROW as $"w")

有界 Over Windows
* 有界的 over window 使用间隔的大小指定
// 有界的事件时间 over window
.window(Over partitionBy $"a" orderBy $"rowtime" preceding 1.minutes as $"w")
// 有界的处理时间 over window
.window(Over partitionBy $"a" orderBy $"proctime" preceding 1.minutes as $"w")
// 有界的事件时间 Row-count over window
.window(Over partitionBy $"a" orderBy $"rowtime" preceding 10.rows as $"w")
// 有界的处理时间 Row-count over window
.window(Over partitionBy $"a" orderBy $"proctime" preceding 10.rows as $"w")
----------------------------------------------------------------------------
SQL 中的 Over Windows
* 用 Over 做窗口聚合时,所有聚合必须在同一窗口上定义,
  也就是说必须是相同的分区、排序和范围
* 目前仅支持在当前行范围之前的窗口
* ORDER BY 必须在单一的时间属性上指定

  SELECT COUNT(amount) OVER (
    PARTITION BY user
    ORDER BY proctime
    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
  )
  FROM Orders;

  val overResultTable = tableEnv.sqlQuery(
      """
        |select
        | id,
        | ts,
        | temperature,
        | count(id) over w,
        | avg(temperature) over w
        |from
        | sensor
        |window w as (partition by id order by rt rows between 2 PRECEDING AND CURRENT ROW)
      """.stripMargin)

----------------------------------------------------------------------------
系统内置函数 一对一 一对多 多对一 多对多
----------------------------------------------------------------------------
UDF-标量函数  一对一
* 用户定义函数是一个重要的特性,它显著地扩展了查询的表达能力
* UDF必须先注册,然后才能在查询中随时用
* 通过调用 registerFunction()方法在TableEnvironment中注册。
  当用户定义的函数被注册时,它被插入到 tableEnv 的函数目录中,
  这样 Table API或SQL解析器就能识别并解析它们
  Table API 不用注册, 直接 new 一个对象就行。
标量函数 Scalar Functions
* 用户定义的标量函数,可以将0、1或多个标量值映射到新的标量值
* 为了定义标量函数, 必须扩展 org.apache.flink.table.functions中
  的基类 ScalarFunction,并实现(一个或多个)求值(eval)方法
* 标量函数的行为由求值方法决定, 求值方法必须公开声明并命名为 eval
  class HashCode(factor:Int) extends ScalarFunction{
    def eval(s:String):Int = {
      s.hashCode + factor
    }
  }
----------------------------------------------------------------------------
UDF-表函数     UDTF  User Defined Table Functions  一对多
* 用户定义的表函数, 也可以将0、1或多个标量值作为输入参数;
  与标量函数不同的是,它可以返回任意数量的行作为输出,而不是单个值
* 为了定义一个表函数,必须扩展 org.apache.flink.table.functions 中
  的基类 TableFunction,并实现(一个或多个)求值(eval)方法
* 表函数的行为由求值方法决定, 求值方法必须公开声明并命名为 eval 且不能有返回值
  class Split(separator:String) extends TableFunction[(String, Int)]{
    def eval(s:String):Unit = {
      s.split(separator).foreach(
        word => collect((word, word.length))
      )
    }
  }
----------------------------------------------------------------------------
UDF-聚合函数   UDAF UDAGGs 多对一
* 用户自定义聚合函数可以把一个表中的数据,聚合成一个标量值
* 用户定义的聚合函数,是通过继承 AggregateFunction 抽象类实现的
  ## 和之前的 common 里的 AggregateFunction 不一样
  UDAGG represents its state using accumulator
  1. createAccumulator()
  2. accumulate(ACC accumulator, [user defined params]...)
  3. getValue(ACC accumulator)

* AggregateFunction 要求必须实现的方法:
  1. createAccumulator()
  2. accumulate(ACC accumulator, [user defined params]...)
  3. getValue(ACC accumulator)
* AggregateFunction 的工作原理如下:
  - 首先,它需要一个累加器,用来保存聚合中间结果的数据结构;
    可以通过 createAccumulator() 方法创建累加器
  - 随后, 对每个输入行调用函数的 accumulate() 方法更新累加器
  - 处理完数据后,调用函数的 getValue() 方法计算返回最终结果
----------------------------------------------------------------------------
UDF-表聚合函数 UDTAF Table Aggregate Functions 多对多
* 用户定义的表聚合函数,可以把一个表中的数据,聚合为具有多行和多列的结果表
* 用户定义的表聚合函数,是通过继承 TableAggregateFunction 抽象类实现的
  UDTAGG represents its state using accumulator
  1. createAccumulator()
  2. accumulate(ACC accumulator, [user defined params]...)
  3. emitValue(ACC accumulator)
* TableAggregateFunction 要求必须实现的方法:
  1. createAccumulator()
  2. accumulate(ACC accumulator, [user defined params]...)
  3. emitValue(ACC accumulator)
* TableAggregateFunction 的工作原理如下:
  - 首先,它需要一个累加器,用来保存聚合中间结果的数据结构;
    可以通过 createAccumulator() 方法创建累加器
  - 随后, 对每个输入行调用函数的 accumulate() 方法更新累加器
  - 处理完数据后,调用函数的 emitValue() 方法计算返回最终结果

## 最直观的应用 ::: TopN